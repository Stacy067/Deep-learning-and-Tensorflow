{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.6"
    },
    "colab": {
      "name": "TF Certificate 3 이미지 분류 (rps) - 실습의 사본",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Stacy067/Deep-learning-and-Tensorflow/blob/main/TF_Certificate_3_%EC%9D%B4%EB%AF%B8%EC%A7%80_%EB%B6%84%EB%A5%98_(rps)_%EC%8B%A4%EC%8A%B5%EC%9D%98_%EC%82%AC%EB%B3%B8.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sCmtzkeGPI2Q"
      },
      "source": [
        "# Category 3\n",
        "\n",
        "Convolution Neural Network (합성곱 신경망)를 활용한 이미지 분류 (Image Classification)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iRSKbgK8PRs5"
      },
      "source": [
        "## 확인"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fc4QcKvRPSj-"
      },
      "source": [
        "1. GPU 옵션 켜져 있는지 확인할 것!!! (수정 - 노트설정 - 하드웨어설정 (GPU))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JNPjnA62PXVn"
      },
      "source": [
        "## 순서"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T463L0aPPX_n"
      },
      "source": [
        "1. **import**: 필요한 모듈 import\n",
        "2. **전처리**: 학습에 필요한 데이터 전처리를 수행합니다.\n",
        "3. **모델링(model)**: 모델을 정의합니다.\n",
        "4. **컴파일(compile)**: 모델을 생성합니다.\n",
        "5. **학습 (fit)**: 모델을 학습시킵니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W1Hj9c1NPbPu"
      },
      "source": [
        "## 문제"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RcvEYUuhPb3f"
      },
      "source": [
        "For this task you will build a classifier for Rock-Paper-Scissors \n",
        "based on the rps dataset.\n",
        "\n",
        "IMPORTANT: Your final layer should be as shown, do not change the\n",
        "provided code, or the tests may fail\n",
        "\n",
        "IMPORTANT: Images will be tested as 150x150 with 3 bytes of color depth\n",
        "So ensure that your input layer is designed accordingly, or the tests\n",
        "may fail. \n",
        "\n",
        "NOTE THAT THIS IS UNLABELLED DATA. \n",
        "You can use the ImageDataGenerator to automatically label it\n",
        "and we have provided some starter code.\n",
        "\n",
        "-------------------------------\n",
        "\n",
        "이 작업에서는 Rock-Paper-Scissors에 대한 분류기를 작성합니다.\n",
        "rps 데이터 셋을 기반으로합니다.\n",
        "\n",
        "중요 : 최종 레이어는 그림과 같아야합니다.\n",
        "\n",
        "중요 : 이미지는 3 바이트 150x150의 컬러사진으로 테스트됩니다.\n",
        "따라서 입력 레이어가 그에 따라 설계되었거나 테스트되었는지 확인하십시오.\n",
        "\n",
        "ImageDataGenerator를 사용하여 자동으로 레이블을 지정할 수 있습니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7C3ewm9XQHgr"
      },
      "source": [
        "-----------------------------------\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U6JA3oYDQJno"
      },
      "source": [
        "# 실습"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TwblClY4QLkb"
      },
      "source": [
        "## STEP 1. import "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_RAc5nAU1LkA"
      },
      "source": [
        "import urllib.request\n",
        "import zipfile\n",
        "import numpy as np\n",
        "from IPython.display import Image\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dropout, Dense\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gzV-1NNX1LkV"
      },
      "source": [
        "## STEP 1. Load Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eOOsLCAj1LkW"
      },
      "source": [
        "가위바위보에 대한 손의 사진을 가지고 `가위`인지, `바위`인지, `보자기`인지 분류하는 `classification` 문제입니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5g1dTl_q1LkX"
      },
      "source": [
        "url = 'https://storage.googleapis.com/download.tensorflow.org/data/rps.zip'\n",
        "urllib.request.urlretrieve(url, 'rps.zip')\n",
        "local_zip = 'rps.zip'\n",
        "zip_ref = zipfile.ZipFile(local_zip, 'r')\n",
        "zip_ref.extractall('tmp/')\n",
        "zip_ref.close()"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QWULnRzA1Lkd"
      },
      "source": [
        "## STEP 2. 전처리 (ImageDataGenerator)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9YoiAzkbQWAO"
      },
      "source": [
        "데이터셋의 경로를 지정해 주세요 (root 폴더의 경로를 지정하여야 합니다.)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R3HJbL5XSDlN"
      },
      "source": [
        "[코드]"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fFJb6A9z1Lkd"
      },
      "source": [
        "TRAINING_DIR = \"tmp/rps/\""
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IjKarwpkQoEr"
      },
      "source": [
        "`ImageDataGenerator`를 정의합니다.\n",
        "\n",
        "다음의 옵션 값들로 Image Aumentation(이미지 변형) 옵션을 적절히 조절해 주세요\n",
        "\n",
        "* `rescale`: 이미지의 픽셀 값을 조정\n",
        "* `rotation_range`: 이미지 회전\n",
        "* `width_shift_range`: 가로 방향으로 이동\n",
        "* `height_shift_range`: 세로 방향으로 이동\n",
        "* `shear_range`: 이미지 굴절\n",
        "* `zoom_range`: 이미지 확대\n",
        "* `horizontal_flip`: 횡 방향으로 이미지 반전\n",
        "* `fill_mode`: 이미지를 이동이나 굴절시켰을 때 빈 픽셀 값에 대하여 값을 채우는 방식\n",
        "* `validation_split`: validation set의 구성 비율"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DGslGzIrSE9U"
      },
      "source": [
        "[코드]"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KemaTeRd1Lkp"
      },
      "source": [
        "training_datagen = ImageDataGenerator(\n",
        "    rescale=1. / 255,\n",
        "    # 위의 옵션 값들을 보고 적절히 대입하여 줍니다.\n",
        "    rotation_range=40,\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    shear_range=0.2,\n",
        "    zoom_range=0.2,\n",
        "    horizontal_flip=True,\n",
        "    fill_mode='nearest',\n",
        "    validation_split=0.2\n",
        "    )"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WZXWPiKY1Lkt"
      },
      "source": [
        "ImageDataGenerator를 잘 만들어 주었다면, `flow_from_directory`로 이미지를 어떻게 공급해 줄 것인가를 지정해 주어야합니다.\n",
        "\n",
        "* train / validation set 전용 generator를 별도로 정의합니다.\n",
        "* `batch_size`를 정의합니다 (128)\n",
        "* `target_size`를 정의합니다. (150 x 150). 이미지를 알아서 타겟 사이즈 만큼 잘라내어 공급합니다.\n",
        "* `class_mode`는 3개 이상의 클래스인 경우 'categorical' 이진 분류의 경우 `binary`를 지정합니다.\n",
        "* `subset`을 지정합니다. (training / validation)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-aaAbYk4RqBd"
      },
      "source": [
        "**training_generator**에 대한 `from_from_directory`를 정의합니다.\n",
        "\n",
        "* 2016 개의 이미지가 출력되어야 합니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LiYxxiZXSGI_"
      },
      "source": [
        "[코드]"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y4gbMm121Lk0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "530bec32-48d0-4f0f-f5b3-1444d35e6eaa"
      },
      "source": [
        "training_generator = training_datagen.flow_from_directory(TRAINING_DIR, \n",
        "                                                          batch_size= 128,\n",
        "                                                          target_size=(150, 150),\n",
        "                                                          class_mode='categorical',                                                          \n",
        "                                                          subset='training',\n",
        "                                                         )"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 2016 images belonging to 3 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r89m3meDRx8k"
      },
      "source": [
        "**validation_generator**에 대한 `from_from_directory`를 정의합니다.\n",
        "\n",
        "* 504 개의 이미지가 출력되어야 합니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Wdicm3XSHT9"
      },
      "source": [
        "[코드]"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_bAYGsIm1aZG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0a6a7b59-0e1d-4c68-dbf8-b25e9ed519fd"
      },
      "source": [
        "validation_generator = training_datagen.flow_from_directory(TRAINING_DIR, \n",
        "                                                            batch_size= 128,\n",
        "                                                            target_size=(150, 150),\n",
        "                                                            class_mode='categorical',\n",
        "                                                            subset='validation', \n",
        "                                                            )"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 504 images belonging to 3 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MKi8C1eoSB52"
      },
      "source": [
        "## STEP 3. 모델 정의 (Sequential)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Iwj8Q1ZR1Llt"
      },
      "source": [
        "model = Sequential([\n",
        "    # Conv2D, MaxPooling2D 조합으로 층을 쌓습니다. 첫번째 입력층의 input_shape은 (150, 150, 3)으로 지정합니다.\n",
        "    Conv2D(64, (3, 3), activation='relu', input_shape=(150, 150, 3)),\n",
        "    MaxPooling2D(2, 2),\n",
        "    Conv2D(64, (3, 3), activation='relu'),\n",
        "    MaxPooling2D(2, 2),\n",
        "    Conv2D(64, (3, 3), activation='relu'),\n",
        "    MaxPooling2D(2, 2),\n",
        "    Conv2D(64, (3, 3), activation='relu'),\n",
        "    MaxPooling2D(2, 2),\n",
        "    # 2D -> 1D로 변환을 위하여 Flatten 합니다.\n",
        "    Flatten(),\n",
        "    # 과적합 방지를 위하여 Dropout을 적용합니다.\n",
        "    Dropout(0.5),\n",
        "    Dense(512, activation='relu'),\n",
        "    # Dense\n",
        "    Dense(3, activation='softmax'),\n",
        "    # Classification을 위한 Softmax \n",
        "    # 출력층의 갯수는 클래스의 갯수와 동일하게 맞춰줍니다 (3개), activation도 잊지마세요!\n",
        "])"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pw_G-lkR1Llv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dd8f3102-96da-465c-f79d-a21468c8e3d6"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d (Conv2D)             (None, 148, 148, 64)      1792      \n",
            "                                                                 \n",
            " max_pooling2d (MaxPooling2D  (None, 74, 74, 64)       0         \n",
            " )                                                               \n",
            "                                                                 \n",
            " conv2d_1 (Conv2D)           (None, 72, 72, 64)        36928     \n",
            "                                                                 \n",
            " max_pooling2d_1 (MaxPooling  (None, 36, 36, 64)       0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " conv2d_2 (Conv2D)           (None, 34, 34, 64)        36928     \n",
            "                                                                 \n",
            " max_pooling2d_2 (MaxPooling  (None, 17, 17, 64)       0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " conv2d_3 (Conv2D)           (None, 15, 15, 64)        36928     \n",
            "                                                                 \n",
            " max_pooling2d_3 (MaxPooling  (None, 7, 7, 64)         0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 3136)              0         \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 3136)              0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 512)               1606144   \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 3)                 1539      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1,720,259\n",
            "Trainable params: 1,720,259\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4NhjbLsU1Ll0"
      },
      "source": [
        "## STEP 4. 컴파일 (compile)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cdjO-SoLSohD"
      },
      "source": [
        "1. `optimizer`는 가장 최적화가 잘되는 알고리즘인 'adam'을 사용합니다.\n",
        "2. `loss`는 무엇을 지정하면 좋을까요? (`categorical_crossentropy` / `sparse_categorical_crossentropy`)\n",
        "3. `metrics`를 'acc' 혹은 'accuracy'로 지정하면, 학습시 정확도를 모니터링 할 수 있습니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yzw3sIaB1Ll0"
      },
      "source": [
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['acc'])"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yJ0gsuoqSv7z"
      },
      "source": [
        "## STEP 5. ModelCheckpoint"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oXHmDZ2aSx4O"
      },
      "source": [
        "`val_loss` 기준으로 epoch 마다 최적의 모델을 저장하기 위하여, ModelCheckpoint를 만듭니다.\n",
        "* `checkpoint_path`는 모델이 저장될 파일 명을 설정합니다.\n",
        "* `ModelCheckpoint`을 선언하고, 적절한 옵션 값을 지정합니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s1i6HUSiSzbL"
      },
      "source": [
        "[코드]"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ogAVTqJAS0um"
      },
      "source": [
        "checkpoint_path = \"shkim_checkpoint.ckpt\"\n",
        "checkpoint = ModelCheckpoint(filepath=checkpoint_path,\n",
        "                             save_weights_only=True,\n",
        "                             save_best_only=True,\n",
        "                             monitor='val_loss',\n",
        "                             verbose=1)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "820Nfvw81Ll5"
      },
      "source": [
        "## STEP 6. 학습 (fit)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ifj14vP71LmA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0c82a371-bc25-42fc-b906-1731828bd680"
      },
      "source": [
        "model.fit(training_generator,\n",
        "          validation_data=(validation_generator),\n",
        "          epochs=25,\n",
        "          callbacks=[checkpoint], \n",
        "          )"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/25\n",
            "16/16 [==============================] - ETA: 0s - loss: 1.1027 - acc: 0.3457\n",
            "Epoch 1: val_loss improved from inf to 1.09578, saving model to shkim_checkpoint.ckpt\n",
            "16/16 [==============================] - 35s 1s/step - loss: 1.1027 - acc: 0.3457 - val_loss: 1.0958 - val_acc: 0.3452\n",
            "Epoch 2/25\n",
            "16/16 [==============================] - ETA: 0s - loss: 1.0474 - acc: 0.4454\n",
            "Epoch 2: val_loss improved from 1.09578 to 1.04454, saving model to shkim_checkpoint.ckpt\n",
            "16/16 [==============================] - 19s 1s/step - loss: 1.0474 - acc: 0.4454 - val_loss: 1.0445 - val_acc: 0.4345\n",
            "Epoch 3/25\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.8809 - acc: 0.5789\n",
            "Epoch 3: val_loss improved from 1.04454 to 0.99346, saving model to shkim_checkpoint.ckpt\n",
            "16/16 [==============================] - 19s 1s/step - loss: 0.8809 - acc: 0.5789 - val_loss: 0.9935 - val_acc: 0.4246\n",
            "Epoch 4/25\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.7254 - acc: 0.6781\n",
            "Epoch 4: val_loss improved from 0.99346 to 0.93513, saving model to shkim_checkpoint.ckpt\n",
            "16/16 [==============================] - 19s 1s/step - loss: 0.7254 - acc: 0.6781 - val_loss: 0.9351 - val_acc: 0.4782\n",
            "Epoch 5/25\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.5904 - acc: 0.7520\n",
            "Epoch 5: val_loss improved from 0.93513 to 0.82831, saving model to shkim_checkpoint.ckpt\n",
            "16/16 [==============================] - 19s 1s/step - loss: 0.5904 - acc: 0.7520 - val_loss: 0.8283 - val_acc: 0.6944\n",
            "Epoch 6/25\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.4438 - acc: 0.8318\n",
            "Epoch 6: val_loss did not improve from 0.82831\n",
            "16/16 [==============================] - 18s 1s/step - loss: 0.4438 - acc: 0.8318 - val_loss: 0.8914 - val_acc: 0.6409\n",
            "Epoch 7/25\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.4020 - acc: 0.8482\n",
            "Epoch 7: val_loss improved from 0.82831 to 0.79363, saving model to shkim_checkpoint.ckpt\n",
            "16/16 [==============================] - 19s 1s/step - loss: 0.4020 - acc: 0.8482 - val_loss: 0.7936 - val_acc: 0.6766\n",
            "Epoch 8/25\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2773 - acc: 0.9048\n",
            "Epoch 8: val_loss improved from 0.79363 to 0.70024, saving model to shkim_checkpoint.ckpt\n",
            "16/16 [==============================] - 19s 1s/step - loss: 0.2773 - acc: 0.9048 - val_loss: 0.7002 - val_acc: 0.7004\n",
            "Epoch 9/25\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2234 - acc: 0.9296\n",
            "Epoch 9: val_loss did not improve from 0.70024\n",
            "16/16 [==============================] - 18s 1s/step - loss: 0.2234 - acc: 0.9296 - val_loss: 0.8077 - val_acc: 0.6488\n",
            "Epoch 10/25\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.1805 - acc: 0.9454\n",
            "Epoch 10: val_loss did not improve from 0.70024\n",
            "16/16 [==============================] - 19s 1s/step - loss: 0.1805 - acc: 0.9454 - val_loss: 1.0905 - val_acc: 0.4980\n",
            "Epoch 11/25\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.1759 - acc: 0.9385\n",
            "Epoch 11: val_loss did not improve from 0.70024\n",
            "16/16 [==============================] - 18s 1s/step - loss: 0.1759 - acc: 0.9385 - val_loss: 0.9992 - val_acc: 0.5813\n",
            "Epoch 12/25\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.1554 - acc: 0.9449\n",
            "Epoch 12: val_loss improved from 0.70024 to 0.59808, saving model to shkim_checkpoint.ckpt\n",
            "16/16 [==============================] - 19s 1s/step - loss: 0.1554 - acc: 0.9449 - val_loss: 0.5981 - val_acc: 0.7996\n",
            "Epoch 13/25\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.1315 - acc: 0.9509\n",
            "Epoch 13: val_loss did not improve from 0.59808\n",
            "16/16 [==============================] - 18s 1s/step - loss: 0.1315 - acc: 0.9509 - val_loss: 1.2402 - val_acc: 0.5417\n",
            "Epoch 14/25\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.1452 - acc: 0.9479\n",
            "Epoch 14: val_loss did not improve from 0.59808\n",
            "16/16 [==============================] - 18s 1s/step - loss: 0.1452 - acc: 0.9479 - val_loss: 0.8088 - val_acc: 0.6726\n",
            "Epoch 15/25\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.0809 - acc: 0.9722\n",
            "Epoch 15: val_loss improved from 0.59808 to 0.58683, saving model to shkim_checkpoint.ckpt\n",
            "16/16 [==============================] - 18s 1s/step - loss: 0.0809 - acc: 0.9722 - val_loss: 0.5868 - val_acc: 0.7679\n",
            "Epoch 16/25\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.0858 - acc: 0.9688\n",
            "Epoch 16: val_loss did not improve from 0.58683\n",
            "16/16 [==============================] - 18s 1s/step - loss: 0.0858 - acc: 0.9688 - val_loss: 0.9378 - val_acc: 0.6885\n",
            "Epoch 17/25\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.0654 - acc: 0.9816\n",
            "Epoch 17: val_loss did not improve from 0.58683\n",
            "16/16 [==============================] - 18s 1s/step - loss: 0.0654 - acc: 0.9816 - val_loss: 0.9406 - val_acc: 0.6806\n",
            "Epoch 18/25\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.0887 - acc: 0.9668\n",
            "Epoch 18: val_loss did not improve from 0.58683\n",
            "16/16 [==============================] - 19s 1s/step - loss: 0.0887 - acc: 0.9668 - val_loss: 1.1557 - val_acc: 0.5635\n",
            "Epoch 19/25\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.0836 - acc: 0.9697\n",
            "Epoch 19: val_loss did not improve from 0.58683\n",
            "16/16 [==============================] - 18s 1s/step - loss: 0.0836 - acc: 0.9697 - val_loss: 1.0917 - val_acc: 0.5575\n",
            "Epoch 20/25\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.0913 - acc: 0.9668\n",
            "Epoch 20: val_loss did not improve from 0.58683\n",
            "16/16 [==============================] - 18s 1s/step - loss: 0.0913 - acc: 0.9668 - val_loss: 1.5803 - val_acc: 0.5397\n",
            "Epoch 21/25\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.0779 - acc: 0.9737\n",
            "Epoch 21: val_loss did not improve from 0.58683\n",
            "16/16 [==============================] - 18s 1s/step - loss: 0.0779 - acc: 0.9737 - val_loss: 1.0553 - val_acc: 0.6091\n",
            "Epoch 22/25\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.0870 - acc: 0.9722\n",
            "Epoch 22: val_loss improved from 0.58683 to 0.53509, saving model to shkim_checkpoint.ckpt\n",
            "16/16 [==============================] - 18s 1s/step - loss: 0.0870 - acc: 0.9722 - val_loss: 0.5351 - val_acc: 0.8254\n",
            "Epoch 23/25\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.0730 - acc: 0.9772\n",
            "Epoch 23: val_loss did not improve from 0.53509\n",
            "16/16 [==============================] - 19s 1s/step - loss: 0.0730 - acc: 0.9772 - val_loss: 0.7039 - val_acc: 0.7619\n",
            "Epoch 24/25\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.0635 - acc: 0.9826\n",
            "Epoch 24: val_loss did not improve from 0.53509\n",
            "16/16 [==============================] - 18s 1s/step - loss: 0.0635 - acc: 0.9826 - val_loss: 0.9195 - val_acc: 0.6508\n",
            "Epoch 25/25\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.0463 - acc: 0.9846\n",
            "Epoch 25: val_loss improved from 0.53509 to 0.49171, saving model to shkim_checkpoint.ckpt\n",
            "16/16 [==============================] - 19s 1s/step - loss: 0.0463 - acc: 0.9846 - val_loss: 0.4917 - val_acc: 0.8353\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f148594f410>"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "shzhTOjAninH"
      },
      "source": [
        "## STEP 7. 학습 완료 후 Load Weights (ModelCheckpoint)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kLqb_6XrMvdq"
      },
      "source": [
        "학습이 완료된 후에는 반드시 `load_weights`를 해주어야 합니다.\n",
        "\n",
        "그렇지 않으면, 열심히 ModelCheckpoint를 만든 의미가 없습니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jcXHQ_pNM_zA"
      },
      "source": [
        "[코드]"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4jO1ucZ9ninH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dcc818f9-90d1-4164-a5e1-8732379627a2"
      },
      "source": [
        "model.load_weights(checkpoint_path)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7f1470508210>"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    }
  ]
}